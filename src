# -*- coding: utf-8 -*-
"""Copy of Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/144EoINaUb7h7OfHaLI1Yp2uOuOL2tgjv

**A- Importing the libraries**
"""

# We will add comments explaining some of the codes (their functions) in the project:

# Libraries for numerical computations and data manipulation
import numpy as np  # For numerical computations and array manipulation
import pandas as pd  # For tabular data manipulation and analysis

# Libraries for visualization
import matplotlib.pyplot as plt  # For creating plots (e.g., line, scatter, bar)
import seaborn as sns  # For advanced visualizations like heatmaps and pairplots

# For dataset splitting and preprocessing
from sklearn.model_selection import train_test_split  # Splits data into training and testing subsets
from sklearn.preprocessing import LabelEncoder  # Encodes categorical labels into numerical format

# Basic evaluation metric
from sklearn.metrics import accuracy_score  # Calculates the proportion of correct predictions

# Advanced evaluation metrics for multi-class classification
from sklearn.metrics import classification_report  # Generates detailed metrics for each class
from sklearn.metrics import confusion_matrix  # Creates a matrix of true vs. predicted class counts
from sklearn.metrics import roc_auc_score  # Calculates the area under the ROC curve
from sklearn.metrics import f1_score  # Combines precision and recall into a single metric
from sklearn.metrics import precision_score  # Measures the proportion of correct positive predictions
from sklearn.metrics import recall_score  # Measures the proportion of actual positives correctly predicted
from sklearn.metrics import log_loss  # Evaluates predicted probabilities against actual labels

# Libraries for model building
from sklearn.ensemble import RandomForestClassifier  # An ensemble method using decision trees for classification
from sklearn.svm import SVC  # Support Vector Classifier for handling high-dimensional data

"""**B- EDA**

1- Load the data sets:
"""

#Load datasets and check their info (types,missing values,etc), and describe (mean,etc)
dataset1 = pd.read_excel('/content/topic1-learningStyles (1).xlsx')
dataset2 = pd.read_csv('/content/topic2-engagement&timeManagement (4).csv')
dataset3 = pd.read_excel('/content/topic3-knowledgeRetention.xlsx')
dataset4 = pd.read_excel('/content/topic4.xlsx')
dataset5 = pd.read_excel('/content/topic5.xlsx')
dataset6 = pd.read_excel('/content/topic6.xlsx')

print(dataset1.head())
print(dataset2.head())
print(dataset3.head())
print(dataset4.head())
print(dataset5.head())
print(dataset6.head())

print("Dataset 1 Info:")
print(dataset1.info())
print(dataset1.describe())

print("\nDataset 2 Info:")
print(dataset2.info())
print(dataset2.describe())

print("\nDataset 3 Info:")
print(dataset3.info())
print(dataset3.describe())

print("\nDataset 4 Info:")
print(dataset4.info())
print(dataset4.describe())

print("\nDataset 5 Info:")
print(dataset5.info())
print(dataset5.describe())

print("\nDataset 6 Info:")
print(dataset6.info())
print(dataset6.describe())

"""2- Find rows and columns Count datasets"""

def print_shape(dataset, dataset_name):
    rows, columns = dataset.shape
    print(f"{dataset_name} - Number of rows: {rows}, Number of columns: {columns}")

# Print number of rows and columns for each dataset
print_shape(dataset1, "Dataset 1")
print_shape(dataset2, "Dataset 2")
print_shape(dataset3, "Dataset 3")
print_shape(dataset4, "Dataset 4")
print_shape(dataset5, "Dataset 5")
print_shape(dataset6, "Dataset 6")

"""3- Clean the datasets"""

#Remove outliers, nulls, etc...
# Function to clean a dataset

def clean_data(dataset, dataset_name):
    print(f"Cleaning {dataset_name}...")
    # Remove rows with null values
    cleaned_dataset = dataset.dropna()

    # Handle numeric outliers using the IQR method
    for column in cleaned_dataset.select_dtypes(include=['number']).columns:
        Q1 = cleaned_dataset[column].quantile(0.25)
        Q3 = cleaned_dataset[column].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        cleaned_dataset = cleaned_dataset[
            (cleaned_dataset[column] >= lower_bound) & (cleaned_dataset[column] <= upper_bound)
        ]

    # Reset index after cleaning
    cleaned_dataset.reset_index(drop=True, inplace=True)

    # Print a summary of cleaning
    print(f"{dataset_name} cleaned: {cleaned_dataset.shape[0]} rows, {cleaned_dataset.shape[1]} columns")
    return cleaned_dataset

# Clean each dataset
dataset1_cleaned = clean_data(dataset1, "Dataset 1")
dataset2_cleaned = clean_data(dataset2, "Dataset 2")
dataset3_cleaned = clean_data(dataset3, "Dataset 3")
dataset4_cleaned = clean_data(dataset4, "Dataset 4")
dataset5_cleaned = clean_data(dataset5, "Dataset 5")
dataset6_cleaned = clean_data(dataset6, "Dataset 6")

"""4-Handle missing values"""

#Filling missing data
# Function to handle missing values in a dataset
def handle_missing_values(dataset, dataset_name):
    print(f"Handling missing values for {dataset_name}...")

    # Fill missing values for numerical columns with the median
    for column in dataset.select_dtypes(include=['number']).columns:
        dataset[column] = dataset[column].fillna(dataset[column].median())

    # Fill missing values for categorical columns with the mode
    for column in dataset.select_dtypes(include=['object', 'category']).columns:
        dataset[column] = dataset[column].fillna(dataset[column].mode()[0])

    print(f"Missing values handled for {dataset_name}.")
    return dataset

# Handle missing values for each dataset
dataset1 = handle_missing_values(dataset1, "Dataset 1")
dataset2 = handle_missing_values(dataset2, "Dataset 2")
dataset3 = handle_missing_values(dataset3, "Dataset 3")
dataset4 = handle_missing_values(dataset4, "Dataset 4")
dataset5 = handle_missing_values(dataset5, "Dataset 5")
dataset6 = handle_missing_values(dataset6, "Dataset 6")

"""5- Visualization of the data"""

#Visualization (Distributions,box plots, and correlation matrix)

# Function to plot distribution, boxplot, and correlation matrix for a dataset
def visualize_data(dataset, dataset_name):
    print(f"Visualizing {dataset_name}...")

    # Filter numeric columns
    numeric_columns = dataset.select_dtypes(include=['number']).columns

    # Plot distributions
    for column in numeric_columns:
        plt.figure(figsize=(8, 4))
        sns.histplot(dataset[column], kde=True)
        plt.title(f"{dataset_name} - Distribution of {column}")
        plt.xlabel(column)
        plt.ylabel("Frequency")
        plt.show()

    # Plot boxplots
    for column in numeric_columns:
        plt.figure(figsize=(8, 4))
        sns.boxplot(x=dataset[column])
        plt.title(f"{dataset_name} - Boxplot of {column}")
        plt.xlabel(column)
        plt.show()

    # Plot correlation matrix if numeric columns exist
    if len(numeric_columns) > 1:
        plt.figure(figsize=(10, 8))
        correlation_matrix = dataset[numeric_columns].corr()
        sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
        plt.title(f"{dataset_name} - Correlation Matrix")
        plt.show()

# Visualize each dataset
visualize_data(dataset1, "Dataset 1")
visualize_data(dataset2, "Dataset 2")
visualize_data(dataset3, "Dataset 3")
visualize_data(dataset4, "Dataset 4")
visualize_data(dataset5, "Dataset 5")
visualize_data(dataset6, "Dataset 6")

"""6- Prepare data for training"""

# Updated Encoding Code with Behavior_Type for Dataset 5

from sklearn.preprocessing import LabelEncoder

# Encoding function
def encode_columns(dataset, columns_to_encode):
    print("\nEncoding dataset...")
    for column in columns_to_encode:
        if column in dataset.columns:
            print(f"Encoding column: {column}")
            le = LabelEncoder()
            dataset[column] = le.fit_transform(dataset[column].astype(str))
        else:
            print(f"Column '{column}' not found. Skipping encoding.")
    print("\nDataset after encoding:")
    print(dataset.head())  # Display the first few rows after encoding
    return dataset

# Apply Encoding

# Learning Preferences Dataset
learning_pref_columns_to_encode = ['Visual Learner (Y/N)', 'Auditory Learner (Y/N)',
                                   'Kinesthetic Learner (Y/N)', 'Read/Write Learner (Y/N)', 'Preferred Materials']
dataset1 = encode_columns(dataset1, learning_pref_columns_to_encode)

# Engagement and Behavior Dataset
engagement_columns_to_encode = ['Engagement Level']
dataset2 = encode_columns(dataset2, engagement_columns_to_encode)

# Retention and Study Habits Dataset
retention_columns_to_encode = ['Retention Type']
dataset3 = encode_columns(dataset3, retention_columns_to_encode)

# Academic Performance Dataset
academic_columns_to_encode = ['Overall_Performance', 'Preference']
dataset4 = encode_columns(dataset4, academic_columns_to_encode)

# Assignment Metrics Dataset
assignment_columns_to_encode = ['Assignment 1 lateness indicator', 'Assignment 2 lateness indicator',
                                 'Assignment 3 lateness indicator']
dataset2 = encode_columns(dataset2, assignment_columns_to_encode)

# Behavior and Scores Dataset (Dataset 5 with `Behavior_Type` fix)
dataset5_columns_to_encode = ['Behavior_Type']
dataset5 = encode_columns(dataset5, dataset5_columns_to_encode)

# Learning Environment Dataset
learning_env_columns_to_encode = ['Learning_Environment', 'Preference_Level']
dataset6 = encode_columns(dataset6, learning_env_columns_to_encode)

from sklearn.preprocessing import MinMaxScaler

# Normalization function
def normalize_all_columns(dataset):
    """
    Normalizes all numeric columns in the dataset using MinMaxScaler.
    Prints column headers before and after normalization.
    """
    print("\nNormalizing dataset...")
    scaler = MinMaxScaler()
    numeric_columns = dataset.select_dtypes(include=['float64', 'int64']).columns  # Select all numeric columns

    if not numeric_columns.empty:
        print("Before Normalization, dataset headers:")
        print(list(dataset.columns))

        # Handle missing values before normalization
        dataset[numeric_columns] = dataset[numeric_columns].fillna(0)

        # Normalize the numeric columns
        before_normalization = dataset[numeric_columns].copy()
        dataset[numeric_columns] = scaler.fit_transform(dataset[numeric_columns])

        # Print before-and-after normalization details
        print("\nBefore Normalization (sample):")
        print(before_normalization.head())
        print("\nAfter Normalization (sample):")
        print(dataset[numeric_columns].head())

        print("After Normalization, dataset headers:")
        print(list(dataset.columns))
    else:
        print("No numeric columns found for normalization.")

    return dataset

# Apply Normalization to Each Dataset

print("\n--- Normalizing Dataset 1: Learning Preferences ---")
dataset1 = normalize_all_columns(dataset1)

print("\n--- Normalizing Dataset 2: Engagement and Behavior ---")
dataset2 = normalize_all_columns(dataset2)

print("\n--- Normalizing Dataset 3: Retention and Study Habits ---")
dataset3 = normalize_all_columns(dataset3)

print("\n--- Normalizing Dataset 4: Academic Performance ---")
dataset4 = normalize_all_columns(dataset4)

print("\n--- Normalizing Dataset 5: Behavior and Scores ---")
dataset5 = normalize_all_columns(dataset5)

print("\n--- Normalizing Dataset 6: Learning Environment ---")
dataset6 = normalize_all_columns(dataset6)

# Display Final Processed Datasets
print("\nFinal Processed Dataset 1 (Sample):")
print(dataset1.head())

print("\nFinal Processed Dataset 2 (Sample):")
print(dataset2.head())

print("\nFinal Processed Dataset 3 (Sample):")
print(dataset3.head())

print("\nFinal Processed Dataset 4 (Sample):")
print(dataset4.head())

print("\nFinal Processed Dataset 5 (Sample):")
print(dataset5.head())

print("\nFinal Processed Dataset 6 (Sample):")
print(dataset6.head())

import pandas as pd

def combine_datasets_as_whole(datasets):
    """
    Combines datasets as a whole by concatenating them column-wise.
    Ensures all data is numeric and handles missing values.

    Args:
        datasets: List of datasets to combine.

    Returns:
        Combined dataset.
    """
    # Concatenate datasets column-wise
    combined_dataset = pd.concat(datasets, axis=1, ignore_index=False)

    # Ensure all data is numeric
    combined_dataset = combined_dataset.apply(pd.to_numeric, errors='coerce')

    # Handle missing values (replace NaN with 0)
    if combined_dataset.isnull().values.any():
        print("Warning: Missing values found in the combined dataset. Filling with 0.")
        combined_dataset.fillna(0, inplace=True)

    # Validate combined dataset
    print("\nFinal Combined Dataset Columns:")
    print(list(combined_dataset.columns))
    print(f"\nCombined Dataset Shape: {combined_dataset.shape}")
    return combined_dataset

# Combine datasets for Neural Network
processed_datasets = [dataset1, dataset2, dataset3, dataset4, dataset5, dataset6]
final_dataset = combine_datasets_as_whole(processed_datasets)

# Validate the Final Dataset
print("\nFinal Combined Dataset Shape:", final_dataset.shape)
print(final_dataset.head())

"""C- Model (Neural Network)

"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, BatchNormalization, LeakyReLU, Softmax
from tensorflow.keras.optimizers import Adam

def build_pretrain_nn(input_dim):
    """
    Builds a neural network for pre-training (autoencoder) with reconstruction loss.
    Args:
        input_dim (int): Number of input features.
    Returns:
        model (tensorflow.keras.Model): Pre-training model.
    """
    input_layer = Input(shape=(input_dim,))
    x = Dense(256)(input_layer)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.01)(x)
    x = Dense(128)(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.01)(x)
    output_layer = Dense(input_dim)(x)  # Reconstruction output
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')  # Reconstruction loss
    return model

def build_clustering_nn(input_dim, n_clusters=10):
    """
    Builds and compiles a clustering model with softmax-based output.
    Args:
        input_dim (int): Number of input features.
        n_clusters (int): Number of clusters.
    Returns:
        model (tensorflow.keras.Model): Clustering model.
    """
    input_layer = Input(shape=(input_dim,))
    x = Dense(256)(input_layer)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.01)(x)
    x = Dense(128)(x)
    x = BatchNormalization()(x)
    x = LeakyReLU(alpha=0.01)(x)
    latent_space = Dense(n_clusters)(x)
    output_layer = Softmax()(latent_space)  # Cluster probabilities
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy')  # Clustering loss
    return model

# Model Initialization
input_dim = final_dataset.shape[1]
n_clusters = 10  # Number of clusters
pretrain_nn = build_pretrain_nn(input_dim)  # Pre-training model
clustering_nn = build_clustering_nn(input_dim, n_clusters)  # Clustering model

from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE  # t-SNE for improved visualization
import matplotlib.pyplot as plt
import numpy as np

# Preprocess Dataset
scaler = MinMaxScaler()
features_combined = scaler.fit_transform(final_dataset.values.astype(np.float32))

# Pre-Training Phase
pretrain_history = pretrain_nn.fit(
    features_combined,  # Input features
    features_combined,  # Reconstruct input features
    epochs=10,
    batch_size=64,
    verbose=1
)

# Extract Latent Features from Pre-Trained Model
pretrained_encoder = Model(
    inputs=pretrain_nn.input,
    outputs=pretrain_nn.layers[-2].output  # Latent representation
)
latent_features_pretrain = pretrained_encoder.predict(features_combined)

# Perform K-Means for Initial Clustering
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
initial_clusters = kmeans.fit_predict(latent_features_pretrain)

# Convert Initial Clusters to One-Hot Encoded Pseudo-Targets
pseudo_targets = np.eye(n_clusters)[initial_clusters]

# Train Clustering Model
history = clustering_nn.fit(
    features_combined,  # Input features
    pseudo_targets,     # Pseudo-targets from K-Means
    validation_split=0.2,
    epochs=100,
    batch_size=64,
    verbose=1
)

# Plot Loss Curves
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Clustering Model Training Loss')
plt.legend()
plt.show()
# Extract Final Latent Features
latent_features = clustering_nn.predict(features_combined)

# Final Clustering with K-Means
kmeans_final = KMeans(n_clusters=n_clusters, random_state=42)
cluster_labels = kmeans_final.fit_predict(latent_features)

# Evaluate Clustering
silhouette_avg = silhouette_score(latent_features, cluster_labels)
print(f'Silhouette Score: {silhouette_avg:.4f}')

# Dimensionality Reduction for Visualization (t-SNE)
tsne = TSNE(n_components=2, perplexity=30, n_iter=300, random_state=42)
latent_2d = tsne.fit_transform(latent_features)

# Visualize Clusters
plt.figure(figsize=(10, 8))
scatter = plt.scatter(
    latent_2d[:, 0],
    latent_2d[:, 1],
    c=cluster_labels,
    cmap='viridis',
    alpha=0.7
)
plt.scatter(
    kmeans_final.cluster_centers_[:, 0],
    kmeans_final.cluster_centers_[:, 1],
    c='red',
    s=200,
    marker='X',
    label='Centroids'
)
plt.title('t-SNE Clustering of Latent Representations')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.legend()
plt.colorbar(scatter, label='Cluster Label')
plt.show()

"""D- LLM"""

pip install --upgrade openai

import openai
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# Set your OpenAI API key here
openai.api_key = 'your-api-key'

# Assuming clustering_nn is already trained and available from your code
model = clustering_nn  # Your trained neural network model

# The final dataset scaler used for training the model
scaler = MinMaxScaler()
scaler.fit(features_combined)  # Assuming features_combined is your full normalized dataset used for training

# Function to process user data (updated to pad data)
def process_user_data(responses):
    # Convert the user responses into numerical data
    data = [
        1 if responses["visual_learning"] == "Yes" else 0,  # Visual Learning
        1 if responses["auditory_learning"] == "Yes" else 0,  # Auditory Learning
        1 if responses["kinesthetic_learning"] == "Yes" else 0,  # Kinesthetic Learning
        1 if responses["reading_writing_learning"] == "Yes" else 0,  # Reading/Writing Learning
        {"Notes": 1, "Hands-on Activities": 2, "Podcasts": 3, "Books": 4, "Videos": 5}.get(responses["preferred_materials"], 0),  # Preferred materials (default to 0 if not found)
        {"Daily": 1, "Weekly": 2, "Never": 0}.get(responses["login_frequency"], 0),  # Login Frequency (default to 0 if not found)
        int(responses.get("content_access", 0)),  # Content Access
        1 if responses["forum_participation"] == "Yes" else 0,  # Forum Participation
        int(responses.get("quiz_review", 0)),  # Quiz Review
        {"High": 2, "Medium": 1, "Low": 0}.get(responses["engagement_level"], 0),  # Engagement Level (default to 0 if not found)
    ]

    # Convert data to numpy array
    data_array = np.array(data).reshape(1, -1)

    # Pad the array with zeros to match the number of features (42)
    padded_data = np.pad(data_array, ((0, 0), (0, 42 - data_array.shape[1])), mode='constant')

    # Normalize the data using the pre-fitted scaler
    normalized_data = scaler.transform(padded_data)

    return normalized_data

# Collect user responses
def collect_responses():
    responses = {}
    responses["visual_learning"] = input("Do you learn better visually? (Yes/No) ").strip().capitalize()
    responses["auditory_learning"] = input("Do you prefer auditory learning, such as listening to lectures or podcasts? (Yes/No) ").strip().capitalize()
    responses["kinesthetic_learning"] = input("Are you a kinesthetic learner, preferring hands-on activities? (Yes/No) ").strip().capitalize()
    responses["reading_writing_learning"] = input("Do you prefer learning by reading and writing? (Yes/No) ").strip().capitalize()
    responses["preferred_materials"] = input("What kind of materials do you prefer when studying? (Notes/Hands-on Activities/Podcasts/Books/Videos) ").strip().capitalize()
    responses["login_frequency"] = input("How often do you log in to the platform? (Daily/Weekly/Never) ").strip().capitalize()
    responses["content_access"] = input("How many pieces of content do you typically read or access? (Enter number) ").strip()
    responses["forum_participation"] = input("Do you actively participate in forums and discussions? (Yes/No) ").strip().capitalize()
    responses["quiz_review"] = input("How many quizzes or tests do you review before submitting them? (Enter number) ").strip()
    responses["engagement_level"] = input("How would you describe your engagement level? (High/Medium/Low) ").strip().capitalize()
    return responses

# Generate prompt for GPT-3.5 based on the responses
def generate_prompt(responses):
    prompt = f"""
    Based on the following responses from a student, provide a personalized study recommendation:
    1. Visual learning: {responses['visual_learning']}
    2. Auditory learning: {responses['auditory_learning']}
    3. Kinesthetic learning: {responses['kinesthetic_learning']}
    4. Reading and writing learning: {responses['reading_writing_learning']}
    5. Preferred materials: {responses['preferred_materials']}
    6. Frequency of logging into platform: {responses['login_frequency']}
    7. Number of content pieces accessed: {responses['content_access']}
    8. Forum participation: {responses['forum_participation']}
    9. Number of quizzes reviewed: {responses['quiz_review']}
    10. Engagement level: {responses['engagement_level']}

    Based on the above information, suggest a personalized study plan for this student.
    """
    return prompt

# Use OpenAI GPT-3.5 or GPT-4 to generate a recommendation (updated with correct usage)
def get_llm_recommendation(prompt):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",  # You can use "gpt-4" if available
        messages=[
            {"role": "system", "content": "You are an expert study advisor."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=150,
        temperature=0.7
    )

    # Extract and return the recommendation
    recommendation = response['choices'][0]['message']['content'].strip()
    return recommendation

# Get neural network prediction
def get_neural_network_prediction(responses):
    user_data = process_user_data(responses)

    # Use the trained clustering model to predict cluster probabilities
    prediction = model.predict(user_data)

    # Get the predicted cluster (highest probability)
    predicted_cluster = np.argmax(prediction)

    # Define recommendations based on cluster prediction
    recommendations = {
        0: "We recommend a focused study plan with more hands-on activities and structured learning.",
        1: "You may benefit from using visual learning tools and group study sessions.",
        2: "Try to incorporate more flexibility in your schedule and use a mix of podcasts and videos for better engagement.",
        3: "You should participate more actively in discussions, which could help boost retention.",
        4: "A mix of reading, quizzes, and consistent revision would help you retain information effectively.",
        5: "We recommend balancing study time across different activities to enhance learning.",
        6: "Focusing on one-on-one tutoring might be helpful for challenging subjects.",
        7: "Incorporate more frequent testing and self-assessment to gauge progress.",
        8: "Group discussions and engaging with peers could be highly beneficial.",
        9: "Relaxed and stress-free environments seem to work well for you, prioritize them while studying."
    }

    return recommendations[predicted_cluster]

# Chatbot class with integrated LLM and Neural Network model
class StudyChatbot:
    def __init__(self):
        self.responses = {}

    def collect_data(self):
        self.responses = collect_responses()

    def generate_recommendation(self):
        prompt = generate_prompt(self.responses)
        llm_recommendation = get_llm_recommendation(prompt)
        print(f"LLM-generated Recommendation: {llm_recommendation}")

        nn_recommendation = get_neural_network_prediction(self.responses)
        print(f"Neural Network-generated Recommendation: {nn_recommendation}")

        # Combine insights from both
        final_recommendation = f"Final Recommendation: {llm_recommendation} \n\nAdditional Insights: {nn_recommendation}"
        print(final_recommendation)

# Example usage
chatbot = StudyChatbot()
chatbot.collect_data()
chatbot.generate_recommendation()
n
